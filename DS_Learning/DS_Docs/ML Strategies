Machine Learning Model

Whatever Algorithm may be -->Inorder to Build the Model we should use Algorithms either Linear Regression,Logistic Regression etc...

****************************************************************NOTE****************************************************************************************************

-----> Algorithm --> For Which kind of data/purpose(what purpose we are taking and what data we are taking based on these and other parameters) and other parameter 
                     like realtime/batch inferencing these 2 are also important things it means this also one of the paramter while Deciding an Algorithm.Suppose,
                     We have One Data then What Algorithm should we need to Decide it means there are some Parameters which will effect on that Decision Making Right!
                     Such that for these set of parameters we need to use this Algorithm and another type of set of parameters we need to use that particular Algorithm
                     that Concept we need to Understand.So, how that will be understand inorder to Understand We are going to discuss what are such type of Parameters.

                 --> Data is first of all what kind of Data it is like Structural Data Or Unstructured Data Or Text Data Or Image Data Or Audio Data Or Video Data etc..
                     What kind of Data it is?

                 --> Then Next what is the Purpose of it it means What purpose Or Ultimate goal of the OR Ultimate goal of doing OR Building a ML Model is What?    
                     Actually, Why we are Building the ML Model and What is the Output of it by Doing and the Purpose what we need to acheive that also involves here
                     that also should be in our consideration.Ok Done We have Built ML Model After Building ML Model We need to Use that ML Model Right and We are Using it like "on the go we are using it" it means after getting input immediately we are getting output and what is the Use-Case of it ML Model
                     Use-Case OR else if it is "not on the go" Just we are going to Deploy it with us Whenever we need it OR Whenever We get the Data then we are trying to do it Batch-Processing then we use different kind of Algorithms for it.

                 --> So,Based on different different things likewise we have many parameters we are come to know whenever we get for each Example/Situation.To  
                     Remember all these parameters is not existed Here and there is no Practice and all.But we have to Read a Lot Here we need to study everytime 
                     more everytime we need to study a Lot.We need to keep on Reading and Try to Debug Ourself by taking our own Use-Case OR Already Deployed Use-Cases
                     OR Already Used Use-Cases So, we have to choose those Use-Cases and Debug it and How they are solving that Use-Cases and What Strategies they have
                     followed and Note that Strategies what they have followed In such situation they have followed like this OR in other situations how they will follow OR If we are in that Place and how we will follow and what strategies we will choose etc.. So,it all comes under how we are Reading the things Here what does it means that Reading and Exploring with our Mind.Totally it's a Mind Work So, it's all about the Mind Work.
************************************************************************************************************************************************************************

Supervised Machine Learning: ----> input and output ---> As we are discussed till now Mostly Supervised ML.So,Mostly we use Supervised ML in our World But in future it may(chances) become Absolate and Reinforcement Learning will Replace it But we are not sure It's my Personal Opinion Exactly it will happen that I can't say But it may have chances.

-----> Algorithm --> Which kind of data/purpose and realtime/batch inferencing ---> Let's Assume We have Choosen an Algorithm by Combining Or considering set of Params
        
        So, by that Algorithm the Phases Actually, In ML Building what Phases we have to do they are as follows:
        --> 1. Extracting Data / Requirement Gathering ---> Mostly it will be a Data we are Extracting.
            ---> Google Data(We will extract the Data from Google) --> General Problems(General Problems can seen by Everyonw and can available in Google)
                                                                   --> In this 1st Step Data Engineering Techniques will also involved like search,clean,Extract etc..
            ---> Specific Problems ---> In Specific Problems the Data Generation is really Very very Hard --> It involves lot of cost --> Highly Costly Data 
                                   ---> Here if we want to prepare a ML Model we need the Data to Train So, that Data we have to Invest like in Phramcitical,etc..
                                   ---> Let's take an example of "Pharmacitical Industry",a minimum of 10,000 rows/examples of a specific combination of chemicals is 
                                        needed. This requires conducting numerous experiments that can take 2-3 years to complete, based on the size of the experiment. Once the data is collected, it is used to develop the machine learning model. However, even with extensive experimentation, there is no guarantee that the model will work properly. This poses a significant financial risk for the company. Therefore, it is important to carefully consider the challenges and costs involved in generating data for specific problems in the industry. Ultimately, the success of the industry depends on its ability to evolve and adapt to these challenges.So, this is how we are generating the Data for Specific Problem.

        --> 2. Labelling Data ---> Here we have 2 Types of Labelling "General Problem Labelling and Specific Problem Labelling"
            ---> General Problems Labelling ---> Like Car detecting Systems,Cycle detecting,Mask detecting etc.. ---> Even a Normal/General People can also Label
                                            ---> It does'nt Involves Cost 
            ---> Specific Problems Labelling ---> Like Glacoma detection System,Brain Tumor Detection etc.. ---> It Requires Experienced Persons in that field
                                             ---> It Involves More Cost.The Labeling Data which we used to Train is already Labeled by Experienced Persons in that
                                                  particular Field by investing more Amount on them for Labelling.So, we need to Build Efficient and high Accuracy
                                                  ML Model Bcs, it involves More Cost and also whenever they Handover such Projects we should be Very Responsible
                                                  in Build High Accuracy and High Performance ML Model.

        ***NOTE***: There are Lot Many Parameters(not W,b) we need to consider Before Architecting the ML Model itself.Based on the situations and lot of discussion
        with the Humans(like Technical Team,Clients,Domain Experts) when we are going with the Talk then we will come to know that what are the purposes of Doing it
        all these comes to know for us then all of those Parameters keeping in Mind. We have to Architect it."Architect" is nothing but just we need to prepare it like
        with  a particular structured Model for a particular problem.For this particular problem Decision Tree and Random Forest are very good thing we will come to know based on the Things.

        --> 3. Architect a Machine Learning Model ---> Once the Labelling is completed we performs "Architect a ML Model" Based on the Things.Below are some Examples
            ---> what kind of data it is?
                Let's Assume:
                ---> Lot of features -> Number of features/columns are Huge
                ---> Data is Limited -> Our data is Limited
                ****The Obvious choice becomes Decision Trees and Random Forest we need to choose.The Obvious choice is that one only that comes out of the Experience. We will come to Understand by that time we can't tell that these 2 things Decision Tree we can't even say Randomly But that experience how we will come to know is Based on how many Problems we are solving and what type of Problems we are solving and what type of Parameters we are considering it will dependsa and also we keep Reading more and explore more then only we will become an expert in ML Strategies.
            ---> what is the purpose?
            ---> Is inference time matters? --> Inference is the process of drawing conclusions or making predictions based on evidence or reasoning. In simpler terms,
                                                it's the act of reaching a conclusion based on what you already know.For example, if you see dark clouds in the sky, you might infer that it's going to rain soon. Or if you hear someone sneezing and coughing, you might infer that they have a cold.Inference can be used in many different contexts, from interpreting literature to analyzing data in scientific research. It's an essential skill for critical thinking and problem solving.
            
            ---> Is model size matters? --> "Model Size" it means our model should be in what size and how many parameters it should have(either less or more parameter)
            Etc..............
            .................
            .................
            ---> Architect --> After Considering Parameters then we are Architecting a particular Architecture.
                           --> For initial step we can adapt any architecture

        PROCEDURE/STRATEGIES/ANALYSIS BEFORE TRAINING ACTUAL ML MODEL:
        -------------------------------------------------------------- 
        ---> 4. Train the ML Model --> After Architecting then we will start to Train the Model.
        Then we will be Using:
            ---> When we have Labelled data with you
                ***NOTE***: These 2 are the Major just two statements But we have Lot more Insights in them and also Below are 2 kinds of Data 1st one is 
                            "Training Data" and 2nd one is "RealTime Data"
                1) How the whole Data distributed? -- It is Distribution of Training Data.
                2) How the whole Data distributed with respect to the realtime data? -- What kind of RealTime Data we are Using Here.

            ---> Example: US --> Military -- Inorder to Build Automated ML Mode System to capture opposite country person or things.The US government captured the Photos of Summer Season Bcs, Wars are done in Winter Season and they Trained Summer Data for their ML Model to Train.When they Bought Images with "Winter"
            Season it won't able to detect things or persons of Opposite one Bcs, In Winter the Wether condition is Snow and white Background it will captured.We 
            already Know in an Image every "Pixel" is a feature.So, In Training it captured Summer Data it consists of Reddish and some other features that are different from Winter.Then how the System will become more Robust to that so, here we need to consider our parameter as "Distribution of Training Data is different and "Distribution of RealTime Data is different" then ML Model Won't Work even we have "very good Accuracy at Training Time" and we have more Data too.That Works with our Distribution of Data Only What we have made it to learn at that manner only it will Learn.If we make it to learn in one way
            and If we ask another New Question then how it will come to know.Not only in this Image Example we have lot many examples something likewise.Therefore,when we are Training our Data OR when we are Training our Model We have to keep in Mind that ok my RealTime Data is this one.So, What we have to do that we  
            include that RealTime Data Distribution also in our "Training Data".Let's Assume In Real-Time throughout the Year these Wars are happening then we have to have the Pictures While in Rain we have to have Pictures in Summer we have to have Pictures in Winter with Different Temperatures with different all kind the Possibilities then if we Build our Model then that Model is a More Robust Model.More Robust in the sense Most Powerful Model.It knows Everything then Whatever we will be given from these then it Works.

            
            In this Case of Example(Let's take same Military example only that we need to Build System for it) then what is our Startegy Our Strategy will be as Follow:
            -------> Every Season photos labelled -- 10000 --->Data we have --->by combining all these seasons.
                       ----> 3 seasons --> 3 types --- In Each Season we again have low,medium,High 
                       ----> 9 different types --- Total we have 9 different Data types we have
                       ----> 10000/9 ==> The Examples It may or may not be as it is while we divide it by 9.But Our Data Distribution will be in the 
                                         Uniform-Distribution.
                       ----> 2000, 100 , 5000, 2000, 900 --- Here, The Data distribution is not Proper.Contribution of the different Phases are not Proper.The Data 
                                                             is Non-Uniform Distribution.Here we are making our model to learn evrything and every season then we have to give good Data for each season.The Data should be Unbiased not Biased on one side it should be Biased on each and every side with more Data.As it is it should not be Distributed Exactly by dividing by 9 for each season.But it should maintain some Uniformity.
                       ----> Inorder to make that Uniformity,
                       Uniformity --> We actually have some "Statistical Formulaes" --> To check Data Uniformity we use a Statistical Formulaes like ---> Chi Score
                       then based on that we need to make the changes and make the Data will be Uniformity by using Statistical Methods Or Approaches.Chi Score gives some score based on that we can find whether that Data is Uniform or Not.

            ***NOTE***: Here 80-90% of work is ours Inorder to Split data and Applying Strategies and Data Uniformity and Architecting etc.. But Domain Experts/SME are also need when we are not Domain specific and also when we are not getting to solve or make decisions we need them definitely in everywhere as a Guidence.

            Here Problem arise with the,
            ----> Different kind of Data in a same Data this kind of Data's are:
                   ---> Train Data --- Training Data is like Practicing ---> Refining/Improving/ Training Data if results by making it proper and adding 
                                                                             missing Values Etc.. we can Perform and Train the Data and again we perform Testing.
                                                                             It means we are trying to Decrease Validation Error Rate to get Better Validation Results. 
                   ---> Validation Data/Holding Data ----> Validation contains distribution of the Realtime data -- The Test which we Perform Before moving to RealTest.
                                                     ----> If the Result which is not Good in validation again we need to Refine our "Train Data" and we Repeat it.
                                                           After Improving Good Validation Accuracy we then go to Testing Data with Realtime Data.
                   ---> Test Data --- It is the Actual Main Testing where we expects Good performance.It is like Final Test/Annual Exam.It is Highly point of interest.


                   PROCEDURE BEFORE MOVING/GIVING TO THE PRODUCTION:
                   -------------------------------------------------
                   Assume:
                   ----> 10000 ---> Labelled Data & Properly distributed or uniformed data
                          -----> split your data ---> (80,20) --- Most of the People will follow this and this also not good for everything

                          8000 ---> Train Data
                          2000 ---> Validation Data ---> Predictions --- Compare this generated predictions with already it is from our labelled Data(Actual values)


                          --- Loss ---> How many examples my ML Model is not doing Well then we will call that as a "Loss/Error"
                                  Lets take/Assume: Whether there is a Human is there or Not in a particcular Photo
                                  Actually there is no human 
                                  If your model predicts there is a human ----> Then it will shot at that Place even there is no Human

                                  How many are true predictions -->

                                  8000 examples ---> Accuracy --- How many of Examples of the Total Train Examples.For how many examples my ML Model is giving True  
                                                                  Predictions is called "Accuracy"

                                  100 examples wrong predict --- Let's ML Model is predicted 100 examples as Wrong like even there is Human predicting no human and 
                                                                 even there is no human it's predicting there is a Human.

                                  Evaluation Metric (We will have Lot many Metrics like "Precision, Recall, F1 Score, Confusion Matrix,loss(MSE,Binary cross Entropy, etc..), Accuracy, etc......) In this Evaluation Metric "Accuracy & Error Rate" is just 2 terms:- We will discuss in further Lastly
                                  Training Accuracy -----> 100% - 1.25% == 98.75%
                                  Training Error rate ---> 100/8000 = 1.25%

                                  Validation Accuracy -----> 100% - 5% == 95%
                                  Validation Error rate ---> 100/2000 = 5%

                          Here we have 2 types of Terms:
                          ---- Train Accuracy  ---> Train Accuracy is the Data OR Accuracy of the Trained Data
                          ---- Validation Accuracy ---> Validation Accuracy is the Data how our model is performing with the unseen Data from our Dataset

                          ----> motive ---> We have to concentrate on Validation Accuracy --- It will be more focused Data Bcs, it will perform Unseen data predictions.
                                                                                              Validation Accuracy is more focused Data than Train Accuracy.



                    ***NOTE****: This Problem of Splitting Data Now a days is also Replaced by AI this is only we called as "Generative Models"
                    -----> Generative ---> will generate validation and test datasets --- This also one of the AI Technique similar to ChatGpt.
                                      ---> It will takes Train Data and gives Validation Data and Test Data.It will generate the Data for us.
                                           It is very very Advance.

        
        ---> Start ML Training Model --> with Train and Validation data

        -------> 5. Testing/RealTime Testing:
                              
                                Monitoring --> In a Monitoring how our Realtime data is performing on a Model.

                                Test Data ----> Realtime Data -- Unlabelled data --- Which is not from our Original Dataset with labelled Data for Testing/Validation.
                                          ----> This New RealTime Data Which we got Here we again Use it For our Future Training Purpose Like similar to ChatGpt.These
                                                Testings like "Beta OR Alpha" Testing etc.. Here, Public privacy data is at Risk 
                                prediction ----> Test Error rate and Test Accuracy 
                                          ----> As there is no Labelled Data.We need to make this prediction is Right or Wrong and how my model is performing in 
                                                Realtime.It is not even in my Training Data and Validation Data But it is coming from Real Photo OR Real situation from RealWorld/Outside then how it works all those will be discussed with Experts/Subject Matter Experts/Data Scientist.

       --------> 6. Repeat the cycle --- It will be Again Repeated from STEP 2 Like "Labelling Data,Architect ML Model,Train the ML Model,Testing/RealTime Testing,
                                         and Repeat the cycle".
                                     --- Initially, the Test Accuracy will be not that much Good.In Production Release 1 the first go we don't say it will be Excellent
                                         there may be Flops.In Next, Iteration we again take this New Data which we got in Production and again We perform the Actions
                                         Likewise we again make New Labelling with Experts and again we need to Architect if there is any new kind of Architectures are came into the Market OR If anyone are prepared any New Research Papers we can use it Any how we architect some Model OR keeping
                                         our New Data aside and try to change intermediately ML Architecture OR Changing Strategies again and again by improving it We can Try in that way too.Everything is a Repetative of the Cycle.ML is a continuous process and Future scope is very High Many things we need a Manual Intervention.
                                
                                
                                           
                                            


           
           




                           
         
            

